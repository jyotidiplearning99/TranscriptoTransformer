{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48233087",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model, load_model\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input, Dense, LayerNormalization, Dropout, MultiHeadAttention, GlobalAveragePooling1D\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/__init__.py:30\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03mTop-level module of TensorFlow. By convention, we refer to this module as\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m`tf` instead of `tensorflow`, following the common practice of importing\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03mthis file with a file generated from [`api_template.__init__.py`](https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/api_template.__init__.py)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-bad-import-order,protected-access,g-import-not-at-top\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdistutils\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_distutils\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_inspect\u001b[39;00m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1002\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:945\u001b[0m, in \u001b[0;36m_find_spec\u001b[0;34m(name, path, target)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/_distutils_hack/__init__.py:90\u001b[0m, in \u001b[0;36mDistutilsMetaFinder.find_spec\u001b[0;34m(self, fullname, path, target)\u001b[0m\n\u001b[1;32m     88\u001b[0m method_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspec_for_\u001b[39m\u001b[38;5;132;01m{fullname}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlocals\u001b[39m())\n\u001b[1;32m     89\u001b[0m method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, method_name, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/_distutils_hack/__init__.py:101\u001b[0m, in \u001b[0;36mDistutilsMetaFinder.spec_for_distutils\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 101\u001b[0m     mod \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msetuptools._distutils\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;66;03m# There are a couple of cases where setuptools._distutils\u001b[39;00m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# may not be present:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;66;03m#   has been loaded. Ref #2980.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;66;03m# In either case, fall back to stdlib behavior.\u001b[39;00m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/setuptools/__init__.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01m_distutils_hack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moverride\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdistutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdistutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DistutilsOptionError\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/_distutils_hack/override.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43m__import__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_distutils_hack\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_override\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/_distutils_hack/__init__.py:70\u001b[0m, in \u001b[0;36mdo_override\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m enabled():\n\u001b[1;32m     69\u001b[0m     warn_distutils_present()\n\u001b[0;32m---> 70\u001b[0m     \u001b[43mensure_local_distutils\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/_distutils_hack/__init__.py:56\u001b[0m, in \u001b[0;36mensure_local_distutils\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m     importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistutils\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# check that submodules load as expected\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m core \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdistutils.core\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_distutils\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m core\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__file__\u001b[39m, core\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__file__\u001b[39m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msetuptools._distutils.log\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmodules\n",
      "File \u001b[0;32m/PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/setuptools/_distutils/core.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebug\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DEBUG\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Mainly import these so setup scripts can \"from distutils.core import\" them.\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdist\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Distribution\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     20\u001b[0m     CCompilerError,\n\u001b[1;32m     21\u001b[0m     DistutilsArgError,\n\u001b[1;32m     22\u001b[0m     DistutilsError,\n\u001b[1;32m     23\u001b[0m     DistutilsSetupError,\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Extension\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/setuptools/_distutils/dist.py:29\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebug\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DEBUG\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     24\u001b[0m     DistutilsArgError,\n\u001b[1;32m     25\u001b[0m     DistutilsClassError,\n\u001b[1;32m     26\u001b[0m     DistutilsModuleError,\n\u001b[1;32m     27\u001b[0m     DistutilsOptionError,\n\u001b[1;32m     28\u001b[0m )\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfancy_getopt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FancyGetopt, translate_longopt\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_environ, rfc822_escape, strtobool\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Regex to define acceptable Distutils command names.  This is not *quite*\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# the same as a Python NAME -- I don't allow leading underscores.  The fact\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# that they're very similar is no coincidence; the default naming scheme is\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# to look for a Python module named after the command.\u001b[39;00m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1002\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:945\u001b[0m, in \u001b[0;36m_find_spec\u001b[0;34m(name, path, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1439\u001b[0m, in \u001b[0;36mfind_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1411\u001b[0m, in \u001b[0;36m_get_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1577\u001b[0m, in \u001b[0;36mfind_spec\u001b[0;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:161\u001b[0m, in \u001b[0;36m_path_isfile\u001b[0;34m(path)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:153\u001b[0m, in \u001b[0;36m_path_is_mode_type\u001b[0;34m(path, mode)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[0;34m(path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, LayerNormalization, Dropout, MultiHeadAttention, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set mixed precision policy\n",
    "set_global_policy('mixed_float16')\n",
    "\n",
    "# Define transformer-based model\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
    "        super(TransformerBlock, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.rate = rate\n",
    "\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [Dense(ff_dim, activation=\"relu\"), Dense(embed_dim)]\n",
    "        )\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(TransformerBlock, self).build(input_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(TransformerBlock, self).get_config()\n",
    "        config.update({\n",
    "            'embed_dim': self.embed_dim,\n",
    "            'num_heads': self.num_heads,\n",
    "            'ff_dim': self.ff_dim,\n",
    "            'rate': self.rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "def create_transformer_model(input_shape, num_heads=2, ff_dim=16, dropout_rate=0.1):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    transformer_block = TransformerBlock(input_shape[-1], num_heads, ff_dim, rate=dropout_rate)\n",
    "    x = transformer_block(inputs)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(20, activation=\"relu\")(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Load metadata and scRNA-seq data\n",
    "metadata_path = '/users/barmanjy/Desktop/1.GSE150949_metaData_with_lineage.txt'\n",
    "data_path = '/users/barmanjy/Desktop/Persister Cell/GSE150949_scRNA.csv'\n",
    "\n",
    "metadata_df = pd.read_csv(metadata_path, sep='\\t')\n",
    "\n",
    "# Function to load scRNA-seq data in chunks\n",
    "def load_data_in_chunks(file_path, chunk_size=1000):\n",
    "    data_chunks = []\n",
    "    for chunk in pd.read_csv(file_path, engine='python', encoding='utf-8', chunksize=chunk_size):\n",
    "        data_chunks.append(chunk)\n",
    "    data = pd.concat(data_chunks, axis=0)\n",
    "    return data\n",
    "\n",
    "scRNA_data = load_data_in_chunks(data_path)\n",
    "\n",
    "# Preprocess the data\n",
    "def preprocess_data(metadata_df, scRNA_data):\n",
    "    persister_metadata = metadata_df[metadata_df['sample_type'].str.endswith(('high', 'low'), na=False)]\n",
    "    persister_metadata = persister_metadata.dropna(subset=['full_cell_barcode'])\n",
    "    persister_metadata = persister_metadata[['full_cell_barcode', 'sample_name', 'sample_type']]\n",
    "    persister_metadata['full_cell_barcode'] = persister_metadata['full_cell_barcode'].str.split('-').str[0]\n",
    "    scRNA_data = scRNA_data.transpose()\n",
    "    scRNA_data.reset_index(inplace=True)\n",
    "    scRNA_data.rename(columns={'index': 'full_cell_barcode'}, inplace=True)\n",
    "    scRNA_data['full_cell_barcode'] = scRNA_data['full_cell_barcode'].str.split('-').str[0]\n",
    "    merged_data = pd.merge(scRNA_data, persister_metadata, on='full_cell_barcode', how='inner')\n",
    "    X = merged_data.drop(columns=['full_cell_barcode', 'sample_name', 'sample_type'])\n",
    "    y = merged_data['sample_type']\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(y)\n",
    "    return X, y, merged_data, label_encoder\n",
    "\n",
    "X, y, merged_data, label_encoder = preprocess_data(metadata_df, scRNA_data)\n",
    "\n",
    "# Split the data into training (80%), validation (10%), and test (10%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Pad the training data and labels\n",
    "def pad_data(X, y, batch_size):\n",
    "    if X.shape[0] % batch_size != 0:\n",
    "        padding_size = batch_size - (X.shape[0] % batch_size)\n",
    "        X = np.pad(X, ((0, padding_size), (0, 0)), mode='constant')\n",
    "        y = np.pad(y, (0, padding_size), mode='constant')\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = pad_data(np.array(X_train, dtype=np.float32), y_train, 8)\n",
    "X_validation, y_validation = pad_data(np.array(X_validation, dtype=np.float32), y_validation, 8)\n",
    "X_test, y_test = pad_data(np.array(X_test, dtype=np.float32), y_test, 8)\n",
    "\n",
    "X_train = X_train[..., np.newaxis]\n",
    "X_validation = X_validation[..., np.newaxis]\n",
    "X_test = X_test[..., np.newaxis]\n",
    "\n",
    "input_shape = (X_train.shape[1], 1)\n",
    "model = create_transformer_model(input_shape)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_persister_model.keras', save_best_only=True, monitor='val_loss')\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=8, validation_data=(X_validation, y_validation), callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "model.save('persister_model.keras')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Generate classification report for the test set\n",
    "y_test_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "print(classification_report(y_test, y_test_pred.flatten(), target_names=label_encoder.classes_))\n",
    "\n",
    "def load_trained_model(model_path='persister_model.keras'):\n",
    "    return load_model(model_path, custom_objects={'TransformerBlock': TransformerBlock})\n",
    "\n",
    "def predict_new_data(new_metadata_path, new_data_path, model, label_encoder, train_columns):\n",
    "    new_metadata_df = pd.read_csv(new_metadata_path, sep='\\t')\n",
    "    new_scRNA_data = load_data_in_chunks(new_data_path)\n",
    "\n",
    "    new_scRNA_data = new_scRNA_data.transpose()\n",
    "    new_scRNA_data.reset_index(inplace=True)\n",
    "    new_scRNA_data.rename(columns={'index': 'full_cell_barcode'}, inplace=True)\n",
    "    new_scRNA_data['full_cell_barcode'] = new_scRNA_data['full_cell_barcode'].str.split('-').str[0]\n",
    "    merged_new_data = pd.merge(new_scRNA_data, new_metadata_df, on='full_cell_barcode', how='inner')\n",
    "    samples_to_predict = merged_new_data[merged_new_data['sample_name'].isin(['14_rep2_high', '14_rep2_low'])]\n",
    "    X_new = samples_to_predict.drop(columns=['full_cell_barcode', 'sample_name', 'sample_type'])\n",
    "    X_new = X_new.reindex(columns=train_columns, fill_value=0)\n",
    "    X_new = np.array(X_new, dtype=np.float32)\n",
    "    X_new = X_new[..., np.newaxis]\n",
    "    \n",
    "    # Create a tf.data.Dataset for prediction\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(X_new)\n",
    "    dataset = dataset.batch(8)\n",
    "\n",
    "    y_pred = (model.predict(dataset) > 0.5).astype(\"int32\")\n",
    "    y_pred_labels = label_encoder.inverse_transform(y_pred.flatten())\n",
    "    return y_pred_labels\n",
    "\n",
    "model = load_trained_model()\n",
    "new_metadata_path = '/users/barmanjy/Desktop/1.GSE150949_metaData_with_lineage.txt'\n",
    "new_data_path = '/users/barmanjy/Desktop/Persister Cell/GSE150949_scRNA.csv'\n",
    "train_columns = list(X.columns)  # Get the column names from the original DataFrame\n",
    "predictions = predict_new_data(new_metadata_path, new_data_path, model, label_encoder, train_columns)\n",
    "\n",
    "# Compare predictions with sample_name '14_rep2_high'\n",
    "comparison_sample = merged_data[merged_data['sample_name'] == '14_rep2_high']\n",
    "X_comparison = comparison_sample.drop(columns=['full_cell_barcode', 'sample_name', 'sample_type'])\n",
    "y_comparison = label_encoder.transform(comparison_sample['sample_type'])\n",
    "\n",
    "X_comparison, y_comparison = pad_data(np.array(X_comparison, dtype=np.float32), y_comparison, 8)\n",
    "X_comparison = X_comparison[..., np.newaxis]\n",
    "\n",
    "y_pred_comparison = (model.predict(X_comparison) > 0.5).astype(\"int32\")\n",
    "print(classification_report(y_comparison, y_pred_comparison.flatten(), target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbd4a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9e7dd84-5bc2-479b-bf00-31b09ae22ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After merge:\n",
      "  full_cell_barcode  RP11-34P13.7  FO538757.2  AP006222.2  RP4-669L17.2  \\\n",
      "0  TAAGCGTCAGCTTCGG           0.0    0.000000         0.0           0.0   \n",
      "1  ATGCGATCAGCTTCGG           0.0    1.234842         0.0           0.0   \n",
      "2  ATTACTCTCGGTCTAA           0.0    2.196514         0.0           0.0   \n",
      "3  CGTCCATTCCCAAGTA           0.0    0.000000         0.0           0.0   \n",
      "4  TGTCCCACAAGCGCTC           0.0    0.000000         0.0           0.0   \n",
      "\n",
      "   RP4-669L17.10  RP5-857K21.4  RP5-857K21.2  RP11-206L10.9  FAM87B  ...  \\\n",
      "0            0.0           0.0           0.0            0.0     0.0  ...   \n",
      "1            0.0           0.0           0.0            0.0     0.0  ...   \n",
      "2            0.0           0.0           0.0            0.0     0.0  ...   \n",
      "3            0.0           0.0           0.0            0.0     0.0  ...   \n",
      "4            0.0           0.0           0.0            0.0     0.0  ...   \n",
      "\n",
      "   nGene   nUMI  orig.ident  percent.mito  sample_id  time_point  \\\n",
      "0   2258   7972     pc9_14d      0.039262          3           3   \n",
      "1   2307   8194     pc9_14d      0.021967          9          14   \n",
      "2   2342  11064     pc9_14d      0.028112          2           3   \n",
      "3   3236  18825     pc9_14d      0.058327          5           7   \n",
      "4   1378   4388     pc9_14d      0.031222          9          14   \n",
      "\n",
      "    sample_name  sample_type                 lineage_barcode  clone_size  \n",
      "0        3_rep2            3  TCAGAGTGAGAGACTCTGACTCTGTGTGAG         1.0  \n",
      "1  14_rep2_high      14_high  TCTGAGACAGTGTGACAGACAGTGAGACAG        51.0  \n",
      "2        3_rep1            3  AGTGAGTGTCACTGTGACTGACTGTGTCAG         3.0  \n",
      "3        7_rep2            7  TGTGAGACAGAGTCTCTGAGAGTGTGACAG         4.0  \n",
      "4  14_rep2_high      14_high  AGACTGTGTGAGTCACACAGTGTGTCTGTC         3.0  \n",
      "\n",
      "[5 rows x 23180 columns]\n",
      "Samples to predict:\n",
      "   full_cell_barcode  RP11-34P13.7  FO538757.2  AP006222.2  RP4-669L17.2  \\\n",
      "1   ATGCGATCAGCTTCGG           0.0    1.234842         0.0           0.0   \n",
      "4   TGTCCCACAAGCGCTC           0.0    0.000000         0.0           0.0   \n",
      "5   CAGATCACATCATCCC           0.0    0.000000         0.0           0.0   \n",
      "12  ACTGATGGTAGCGATG           0.0    0.000000         0.0           0.0   \n",
      "13  ACTGTCCCATCCAACA           0.0    0.000000         0.0           0.0   \n",
      "\n",
      "    RP4-669L17.10  RP5-857K21.4  RP5-857K21.2  RP11-206L10.9  FAM87B  ...  \\\n",
      "1             0.0           0.0           0.0            0.0     0.0  ...   \n",
      "4             0.0           0.0           0.0            0.0     0.0  ...   \n",
      "5             0.0           0.0           0.0            0.0     0.0  ...   \n",
      "12            0.0           0.0           0.0            0.0     0.0  ...   \n",
      "13            0.0           0.0           0.0            0.0     0.0  ...   \n",
      "\n",
      "    nGene  nUMI  orig.ident  percent.mito  sample_id  time_point  \\\n",
      "1    2307  8194     pc9_14d      0.021967          9          14   \n",
      "4    1378  4388     pc9_14d      0.031222          9          14   \n",
      "5    2565  9749     pc9_14d      0.017951          9          14   \n",
      "12   2523  8521     pc9_14d      0.026757          6          14   \n",
      "13   2319  9051     pc9_14d      0.022318          9          14   \n",
      "\n",
      "     sample_name  sample_type  \\\n",
      "1   14_rep2_high      14_high   \n",
      "4   14_rep2_high      14_high   \n",
      "5   14_rep2_high      14_high   \n",
      "12  14_rep1_high      14_high   \n",
      "13  14_rep2_high      14_high   \n",
      "\n",
      "                                      lineage_barcode  clone_size  \n",
      "1                      TCTGAGACAGTGTGACAGACAGTGAGACAG        51.0  \n",
      "4                      AGACTGTGTGAGTCACACAGTGTGTCTGTC         3.0  \n",
      "5   TGTCTCACTGTGACTGAGTCTGACTGTGTG,TGTGTGTGTCACTGT...         1.0  \n",
      "12                     ACACACTCAGACAGACTGAGTGAGAGTGAG         5.0  \n",
      "13                     TCTGTGACTGTCAGTCTGTGTGAGTGTCAG         5.0  \n",
      "\n",
      "[5 rows x 23180 columns]\n",
      "X_new after reindexing:\n",
      "    RP11-34P13.7  FO538757.2  AP006222.2  RP4-669L17.2  RP4-669L17.10  \\\n",
      "1            0.0    1.234842         0.0           0.0            0.0   \n",
      "4            0.0    0.000000         0.0           0.0            0.0   \n",
      "5            0.0    0.000000         0.0           0.0            0.0   \n",
      "12           0.0    0.000000         0.0           0.0            0.0   \n",
      "13           0.0    0.000000         0.0           0.0            0.0   \n",
      "\n",
      "    RP5-857K21.4  RP5-857K21.2  RP11-206L10.9  FAM87B  LINC00115  ...  \\\n",
      "1            0.0           0.0            0.0     0.0        0.0  ...   \n",
      "4            0.0           0.0            0.0     0.0        0.0  ...   \n",
      "5            0.0           0.0            0.0     0.0        0.0  ...   \n",
      "12           0.0           0.0            0.0     0.0        0.0  ...   \n",
      "13           0.0           0.0            0.0     0.0        0.0  ...   \n",
      "\n",
      "    BX004987.4  AC145212.4  AC145212.2  AC011043.1  AL592183.1  AC007325.4  \\\n",
      "1          0.0         0.0         0.0         0.0         0.0         0.0   \n",
      "4          0.0         0.0         0.0         0.0         0.0         0.0   \n",
      "5          0.0         0.0         0.0         0.0         0.0         0.0   \n",
      "12         0.0         0.0         0.0         0.0         0.0         0.0   \n",
      "13         0.0         0.0         0.0         0.0         0.0         0.0   \n",
      "\n",
      "    AC007325.2  AL354822.1  AC004556.1  AC240274.1  \n",
      "1          0.0         0.0         0.0         0.0  \n",
      "4          0.0         0.0         0.0         0.0  \n",
      "5          0.0         0.0         0.0         0.0  \n",
      "12         0.0         0.0         0.0         0.0  \n",
      "13         0.0         0.0         0.0         0.0  \n",
      "\n",
      "[5 rows x 23168 columns]\n",
      "New data shape after reindexing and reshaping: (10, 23168, 1)\n",
      "---Dataset\n",
      "<_BatchDataset element_spec=TensorSpec(shape=(None, 23168, 1), dtype=tf.float32, name=None)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-16 11:23:25.247981: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 6s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 42s/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     14_high       1.00      1.00      1.00        16\n",
      "\n",
      "    accuracy                           1.00        16\n",
      "   macro avg       1.00      1.00      1.00        16\n",
      "weighted avg       1.00      1.00      1.00        16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def preprocess_new_data(metadata_path, data_path, train_columns):\n",
    "    new_metadata_df = pd.read_csv(metadata_path, sep='\\t')\n",
    "    new_scRNA_data = load_data_in_chunks(data_path).transpose()\n",
    "\n",
    "    new_metadata_df['full_cell_barcode'] = new_metadata_df['full_cell_barcode'].str.split('-').str[0]\n",
    "    new_scRNA_data.reset_index(inplace=True)\n",
    "    new_scRNA_data.rename(columns={'index': 'full_cell_barcode'}, inplace=True)\n",
    "    new_scRNA_data['full_cell_barcode'] = new_scRNA_data['full_cell_barcode'].str.split('-').str[0]\n",
    "\n",
    "    merged_new_data = pd.merge(new_scRNA_data, new_metadata_df, on='full_cell_barcode', how='inner')\n",
    "    print('After merge:')\n",
    "    print(merged_new_data.head())\n",
    "\n",
    "    # Adjust the filtering condition to capture 'high' or 'low' within 'sample_type'\n",
    "    samples_to_predict = merged_new_data[merged_new_data['sample_type'].str.contains('high|low', case=False)]\n",
    "    print('Samples to predict:')\n",
    "    print(samples_to_predict.head())\n",
    "\n",
    "    X_new = samples_to_predict.drop(columns=['full_cell_barcode', 'sample_name', 'sample_type'])\n",
    "    X_new = X_new.reindex(columns=train_columns, fill_value=0)\n",
    "    print('X_new after reindexing:')\n",
    "    print(X_new.head())\n",
    "\n",
    "    return np.array(X_new, dtype=np.float32)\n",
    "\n",
    "def predict_new_data(metadata_path, data_path, model, label_encoder, train_columns):\n",
    "    X_new = preprocess_new_data(metadata_path, data_path, train_columns)\n",
    "    X_new = X_new[..., np.newaxis]\n",
    "    print(f\"New data shape after reindexing and reshaping: {X_new.shape}\")\n",
    "\n",
    "    # Create a tf.data.Dataset for prediction\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(X_new)\n",
    "    dataset = dataset.batch(8)\n",
    "    print('---Dataset')\n",
    "    print(dataset)\n",
    "\n",
    "    # Ensure dataset has at least one element by checking its length\n",
    "    if len(list(dataset)) == 0:\n",
    "        raise ValueError(\"The dataset is empty. Please ensure that the dataset contains elements.\")\n",
    "\n",
    "    # Perform prediction\n",
    "    y_pred = (model.predict(dataset) > 0.5).astype(\"int32\")\n",
    "    y_pred_labels = label_encoder.inverse_transform(y_pred.flatten())\n",
    "    return y_pred_labels\n",
    "\n",
    "# Load the model and make predictions\n",
    "model = load_trained_model()\n",
    "predictions = predict_new_data(new_metadata_path, new_data_path, model, label_encoder, train_columns)\n",
    "\n",
    "# Compare predictions with sample_type '14_high'\n",
    "comparison_sample = merged_data[merged_data['sample_type'] == '14_high']\n",
    "X_comparison = comparison_sample.drop(columns=['full_cell_barcode', 'sample_name', 'sample_type'])\n",
    "y_comparison = label_encoder.transform(comparison_sample['sample_type'])\n",
    "\n",
    "X_comparison, y_comparison = pad_data(np.array(X_comparison, dtype=np.float32), y_comparison, 8)\n",
    "X_comparison = X_comparison[..., np.newaxis]\n",
    "\n",
    "y_pred_comparison = (model.predict(X_comparison) > 0.5).astype(\"int32\")\n",
    "print(classification_report(y_comparison, y_pred_comparison.flatten(), target_names=label_encoder.classes_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792da72b-4a10-4cab-aafb-7b140837ba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "--code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d6e546a-40c5-49b3-9c64-69d1de65ea0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 66s/step - accuracy: 1.0000 - loss: 0.6931 - val_accuracy: 1.0000 - val_loss: 0.6927\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 61s/step - accuracy: 1.0000 - loss: 0.6927 - val_accuracy: 1.0000 - val_loss: 0.6922\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 60s/step - accuracy: 1.0000 - loss: 0.6922 - val_accuracy: 1.0000 - val_loss: 0.6917\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 62s/step - accuracy: 1.0000 - loss: 0.6917 - val_accuracy: 1.0000 - val_loss: 0.6912\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 61s/step - accuracy: 1.0000 - loss: 0.6912 - val_accuracy: 1.0000 - val_loss: 0.6907\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 60s/step - accuracy: 1.0000 - loss: 0.6907 - val_accuracy: 1.0000 - val_loss: 0.6902\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 60s/step - accuracy: 1.0000 - loss: 0.6902 - val_accuracy: 1.0000 - val_loss: 0.6897\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 60s/step - accuracy: 1.0000 - loss: 0.6897 - val_accuracy: 1.0000 - val_loss: 0.6892\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 60s/step - accuracy: 1.0000 - loss: 0.6892 - val_accuracy: 1.0000 - val_loss: 0.6888\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 57s/step - accuracy: 1.0000 - loss: 0.6888 - val_accuracy: 1.0000 - val_loss: 0.6883\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 23s/step - accuracy: 1.0000 - loss: 0.6883\n",
      "Test Loss: 0.6882762312889099\n",
      "Test Accuracy: 1.0\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20s/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     14_high       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           1.00         8\n",
      "   macro avg       1.00      1.00      1.00         8\n",
      "weighted avg       1.00      1.00      1.00         8\n",
      "\n",
      "After merge:\n",
      "  full_cell_barcode  RP11-34P13.7  FO538757.2  AP006222.2  RP4-669L17.2  \\\n",
      "0  TAAGCGTCAGCTTCGG           0.0    0.000000         0.0           0.0   \n",
      "1  ATGCGATCAGCTTCGG           0.0    1.234842         0.0           0.0   \n",
      "2  ATTACTCTCGGTCTAA           0.0    2.196514         0.0           0.0   \n",
      "3  CGTCCATTCCCAAGTA           0.0    0.000000         0.0           0.0   \n",
      "4  TGTCCCACAAGCGCTC           0.0    0.000000         0.0           0.0   \n",
      "\n",
      "   RP4-669L17.10  RP5-857K21.4  RP5-857K21.2  RP11-206L10.9  FAM87B  ...  \\\n",
      "0            0.0           0.0           0.0            0.0     0.0  ...   \n",
      "1            0.0           0.0           0.0            0.0     0.0  ...   \n",
      "2            0.0           0.0           0.0            0.0     0.0  ...   \n",
      "3            0.0           0.0           0.0            0.0     0.0  ...   \n",
      "4            0.0           0.0           0.0            0.0     0.0  ...   \n",
      "\n",
      "   nGene   nUMI  orig.ident  percent.mito  sample_id  time_point  \\\n",
      "0   2258   7972     pc9_14d      0.039262          3           3   \n",
      "1   2307   8194     pc9_14d      0.021967          9          14   \n",
      "2   2342  11064     pc9_14d      0.028112          2           3   \n",
      "3   3236  18825     pc9_14d      0.058327          5           7   \n",
      "4   1378   4388     pc9_14d      0.031222          9          14   \n",
      "\n",
      "    sample_name  sample_type                 lineage_barcode  clone_size  \n",
      "0        3_rep2            3  TCAGAGTGAGAGACTCTGACTCTGTGTGAG         1.0  \n",
      "1  14_rep2_high      14_high  TCTGAGACAGTGTGACAGACAGTGAGACAG        51.0  \n",
      "2        3_rep1            3  AGTGAGTGTCACTGTGACTGACTGTGTCAG         3.0  \n",
      "3        7_rep2            7  TGTGAGACAGAGTCTCTGAGAGTGTGACAG         4.0  \n",
      "4  14_rep2_high      14_high  AGACTGTGTGAGTCACACAGTGTGTCTGTC         3.0  \n",
      "\n",
      "[5 rows x 23180 columns]\n",
      "Samples to predict:\n",
      "   full_cell_barcode  RP11-34P13.7  FO538757.2  AP006222.2  RP4-669L17.2  \\\n",
      "1   ATGCGATCAGCTTCGG           0.0    1.234842         0.0           0.0   \n",
      "4   TGTCCCACAAGCGCTC           0.0    0.000000         0.0           0.0   \n",
      "5   CAGATCACATCATCCC           0.0    0.000000         0.0           0.0   \n",
      "12  ACTGATGGTAGCGATG           0.0    0.000000         0.0           0.0   \n",
      "13  ACTGTCCCATCCAACA           0.0    0.000000         0.0           0.0   \n",
      "\n",
      "    RP4-669L17.10  RP5-857K21.4  RP5-857K21.2  RP11-206L10.9  FAM87B  ...  \\\n",
      "1             0.0           0.0           0.0            0.0     0.0  ...   \n",
      "4             0.0           0.0           0.0            0.0     0.0  ...   \n",
      "5             0.0           0.0           0.0            0.0     0.0  ...   \n",
      "12            0.0           0.0           0.0            0.0     0.0  ...   \n",
      "13            0.0           0.0           0.0            0.0     0.0  ...   \n",
      "\n",
      "    nGene  nUMI  orig.ident  percent.mito  sample_id  time_point  \\\n",
      "1    2307  8194     pc9_14d      0.021967          9          14   \n",
      "4    1378  4388     pc9_14d      0.031222          9          14   \n",
      "5    2565  9749     pc9_14d      0.017951          9          14   \n",
      "12   2523  8521     pc9_14d      0.026757          6          14   \n",
      "13   2319  9051     pc9_14d      0.022318          9          14   \n",
      "\n",
      "     sample_name  sample_type  \\\n",
      "1   14_rep2_high      14_high   \n",
      "4   14_rep2_high      14_high   \n",
      "5   14_rep2_high      14_high   \n",
      "12  14_rep1_high      14_high   \n",
      "13  14_rep2_high      14_high   \n",
      "\n",
      "                                      lineage_barcode  clone_size  \n",
      "1                      TCTGAGACAGTGTGACAGACAGTGAGACAG        51.0  \n",
      "4                      AGACTGTGTGAGTCACACAGTGTGTCTGTC         3.0  \n",
      "5   TGTCTCACTGTGACTGAGTCTGACTGTGTG,TGTGTGTGTCACTGT...         1.0  \n",
      "12                     ACACACTCAGACAGACTGAGTGAGAGTGAG         5.0  \n",
      "13                     TCTGTGACTGTCAGTCTGTGTGAGTGTCAG         5.0  \n",
      "\n",
      "[5 rows x 23180 columns]\n",
      "X_new after reindexing:\n",
      "    RP11-34P13.7  FO538757.2  AP006222.2  RP4-669L17.2  RP4-669L17.10  \\\n",
      "1            0.0    1.234842         0.0           0.0            0.0   \n",
      "4            0.0    0.000000         0.0           0.0            0.0   \n",
      "5            0.0    0.000000         0.0           0.0            0.0   \n",
      "12           0.0    0.000000         0.0           0.0            0.0   \n",
      "13           0.0    0.000000         0.0           0.0            0.0   \n",
      "\n",
      "    RP5-857K21.4  RP5-857K21.2  RP11-206L10.9  FAM87B  LINC00115  ...  \\\n",
      "1            0.0           0.0            0.0     0.0        0.0  ...   \n",
      "4            0.0           0.0            0.0     0.0        0.0  ...   \n",
      "5            0.0           0.0            0.0     0.0        0.0  ...   \n",
      "12           0.0           0.0            0.0     0.0        0.0  ...   \n",
      "13           0.0           0.0            0.0     0.0        0.0  ...   \n",
      "\n",
      "    BX004987.4  AC145212.4  AC145212.2  AC011043.1  AL592183.1  AC007325.4  \\\n",
      "1          0.0         0.0         0.0         0.0         0.0         0.0   \n",
      "4          0.0         0.0         0.0         0.0         0.0         0.0   \n",
      "5          0.0         0.0         0.0         0.0         0.0         0.0   \n",
      "12         0.0         0.0         0.0         0.0         0.0         0.0   \n",
      "13         0.0         0.0         0.0         0.0         0.0         0.0   \n",
      "\n",
      "    AC007325.2  AL354822.1  AC004556.1  AC240274.1  \n",
      "1          0.0         0.0         0.0         0.0  \n",
      "4          0.0         0.0         0.0         0.0  \n",
      "5          0.0         0.0         0.0         0.0  \n",
      "12         0.0         0.0         0.0         0.0  \n",
      "13         0.0         0.0         0.0         0.0  \n",
      "\n",
      "[5 rows x 23168 columns]\n",
      "New data shape after reindexing and reshaping: (10, 23168, 1)\n",
      "---Dataset\n",
      "<_BatchDataset element_spec=TensorSpec(shape=(None, 23168, 1), dtype=tf.float32, name=None)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-16 11:58:40.422025: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f6b9f483a30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 21s/stepWARNING:tensorflow:6 out of the last 7 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f6b9f483a30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 5s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 41s/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     14_high       1.00      1.00      1.00        16\n",
      "\n",
      "    accuracy                           1.00        16\n",
      "   macro avg       1.00      1.00      1.00        16\n",
      "weighted avg       1.00      1.00      1.00        16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, LayerNormalization, Dropout, MultiHeadAttention, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set mixed precision policy\n",
    "set_global_policy('mixed_float16')\n",
    "\n",
    "# Define transformer-based model\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
    "        super(TransformerBlock, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.rate = rate\n",
    "\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [Dense(ff_dim, activation=\"relu\"), Dense(embed_dim)]\n",
    "        )\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(TransformerBlock, self).build(input_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(TransformerBlock, self).get_config()\n",
    "        config.update({\n",
    "            'embed_dim': self.embed_dim,\n",
    "            'num_heads': self.num_heads,\n",
    "            'ff_dim': self.ff_dim,\n",
    "            'rate': self.rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "def create_transformer_model(input_shape, num_heads=2, ff_dim=16, dropout_rate=0.1):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    transformer_block = TransformerBlock(input_shape[-1], num_heads, ff_dim, rate=dropout_rate)\n",
    "    x = transformer_block(inputs)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(20, activation=\"relu\")(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Load metadata and scRNA-seq data\n",
    "metadata_path = '/users/barmanjy/Desktop/1.GSE150949_metaData_with_lineage.txt'\n",
    "data_path = '/users/barmanjy/Desktop/Persister Cell/GSE150949_scRNA.csv'\n",
    "\n",
    "metadata_df = pd.read_csv(metadata_path, sep='\\t')\n",
    "\n",
    "# Function to load scRNA-seq data in chunks\n",
    "def load_data_in_chunks(file_path, chunk_size=1000):\n",
    "    data_chunks = []\n",
    "    for chunk in pd.read_csv(file_path, engine='python', encoding='utf-8', chunksize=chunk_size):\n",
    "        data_chunks.append(chunk)\n",
    "    data = pd.concat(data_chunks, axis=0)\n",
    "    return data\n",
    "\n",
    "scRNA_data = load_data_in_chunks(data_path).transpose()\n",
    "\n",
    "# Preprocess the data\n",
    "def preprocess_data(metadata_df, scRNA_data):\n",
    "    metadata_df['full_cell_barcode'] = metadata_df['full_cell_barcode'].str.split('-').str[0]\n",
    "    scRNA_data.reset_index(inplace=True)\n",
    "    scRNA_data.rename(columns={'index': 'full_cell_barcode'}, inplace=True)\n",
    "    scRNA_data['full_cell_barcode'] = scRNA_data['full_cell_barcode'].str.split('-').str[0]\n",
    "    \n",
    "    persister_metadata = metadata_df[metadata_df['sample_type'].str.endswith(('high', 'low'), na=False)]\n",
    "    persister_metadata = persister_metadata.dropna(subset=['full_cell_barcode'])\n",
    "    persister_metadata = persister_metadata[['full_cell_barcode', 'sample_name', 'sample_type']]\n",
    "    \n",
    "    merged_data = pd.merge(scRNA_data, persister_metadata, on='full_cell_barcode', how='inner')\n",
    "    X = merged_data.drop(columns=['full_cell_barcode', 'sample_name', 'sample_type'])\n",
    "    y = merged_data['sample_type']\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(y)\n",
    "    return X, y, merged_data, label_encoder\n",
    "\n",
    "X, y, merged_data, label_encoder = preprocess_data(metadata_df, scRNA_data)\n",
    "\n",
    "# Split the data into training (80%), validation (10%), and test (10%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Pad the training data and labels\n",
    "def pad_data(X, y, batch_size):\n",
    "    if X.shape[0] % batch_size != 0:\n",
    "        padding_size = batch_size - (X.shape[0] % batch_size)\n",
    "        X = np.pad(X, ((0, padding_size), (0, 0)), mode='constant')\n",
    "        y = np.pad(y, (0, padding_size), mode='constant')\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = pad_data(np.array(X_train, dtype=np.float32), y_train, 8)\n",
    "X_validation, y_validation = pad_data(np.array(X_validation, dtype=np.float32), y_validation, 8)\n",
    "X_test, y_test = pad_data(np.array(X_test, dtype=np.float32), y_test, 8)\n",
    "\n",
    "X_train = X_train[..., np.newaxis]\n",
    "X_validation = X_validation[..., np.newaxis]\n",
    "X_test = X_test[..., np.newaxis]\n",
    "\n",
    "input_shape = (X_train.shape[1], 1)\n",
    "model = create_transformer_model(input_shape)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_persister_model.keras', save_best_only=True, monitor='val_loss')\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=8, validation_data=(X_validation, y_validation), callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "model.save('persister_model.keras')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Generate classification report for the test set\n",
    "y_test_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "print(classification_report(y_test, y_test_pred.flatten(), target_names=label_encoder.classes_))\n",
    "\n",
    "def load_trained_model(model_path='persister_model.keras'):\n",
    "    return load_model(model_path, custom_objects={'TransformerBlock': TransformerBlock})\n",
    "\n",
    "def preprocess_new_data(metadata_path, data_path, train_columns):\n",
    "    new_metadata_df = pd.read_csv(metadata_path, sep='\\t')\n",
    "    new_scRNA_data = load_data_in_chunks(data_path).transpose()\n",
    "\n",
    "    new_metadata_df['full_cell_barcode'] = new_metadata_df['full_cell_barcode'].str.split('-').str[0]\n",
    "    new_scRNA_data.reset_index(inplace=True)\n",
    "    new_scRNA_data.rename(columns={'index': 'full_cell_barcode'}, inplace=True)\n",
    "    new_scRNA_data['full_cell_barcode'] = new_scRNA_data['full_cell_barcode'].str.split('-').str[0]\n",
    "\n",
    "    merged_new_data = pd.merge(new_scRNA_data, new_metadata_df, on='full_cell_barcode', how='inner')\n",
    "    print('After merge:')\n",
    "    print(merged_new_data.head())\n",
    "\n",
    "    # Adjust the filtering condition to capture 'high' or 'low' within 'sample_type'\n",
    "    samples_to_predict = merged_new_data[merged_new_data['sample_type'].str.contains('high|low', case=False)]\n",
    "    print('Samples to predict:')\n",
    "    print(samples_to_predict.head())\n",
    "\n",
    "    X_new = samples_to_predict.drop(columns=['full_cell_barcode', 'sample_name', 'sample_type'])\n",
    "    X_new = X_new.reindex(columns=train_columns, fill_value=0)\n",
    "    print('X_new after reindexing:')\n",
    "    print(X_new.head())\n",
    "\n",
    "    return np.array(X_new, dtype=np.float32)\n",
    "\n",
    "def predict_new_data(metadata_path, data_path, model, label_encoder, train_columns):\n",
    "    X_new = preprocess_new_data(metadata_path, data_path, train_columns)\n",
    "    X_new = X_new[..., np.newaxis]\n",
    "    print(f\"New data shape after reindexing and reshaping: {X_new.shape}\")\n",
    "\n",
    "    # Create a tf.data.Dataset for prediction\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(X_new)\n",
    "    dataset = dataset.batch(8)\n",
    "    print('---Dataset')\n",
    "    print(dataset)\n",
    "\n",
    "    # Ensure dataset has at least one element by checking its length\n",
    "    if len(list(dataset)) == 0:\n",
    "        raise ValueError(\"The dataset is empty. Please ensure that the dataset contains elements.\")\n",
    "\n",
    "    # Perform prediction\n",
    "    y_pred = (model.predict(dataset) > 0.5).astype(\"int32\")\n",
    "    y_pred_labels = label_encoder.inverse_transform(y_pred.flatten())\n",
    "    return y_pred_labels\n",
    "\n",
    "# Load the model and make predictions\n",
    "model = load_trained_model()\n",
    "predictions = predict_new_data(new_metadata_path, new_data_path, model, label_encoder, train_columns)\n",
    "\n",
    "# Compare predictions with sample_type '14_high'\n",
    "comparison_sample = merged_data[merged_data['sample_type'] == '14_high']\n",
    "X_comparison = comparison_sample.drop(columns=['full_cell_barcode', 'sample_name', 'sample_type'])\n",
    "y_comparison = label_encoder.transform(comparison_sample['sample_type'])\n",
    "\n",
    "X_comparison, y_comparison = pad_data(np.array(X_comparison, dtype=np.float32), y_comparison, 8)\n",
    "X_comparison = X_comparison[..., np.newaxis]\n",
    "\n",
    "y_pred_comparison = (model.predict(X_comparison) > 0.5).astype(\"int32\")\n",
    "print(classification_report(y_comparison, y_pred_comparison.flatten(), target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d129df0-cb86-4686-be60-f8031a785422",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
